{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f6e2e-0aa3-4f0f-924b-8a4ef29d1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def deriv_relu(z):\n",
    "    return (z > 0).astype(int)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def deriv_softmax(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def categorical_cross_entropy_loss(pred, targets):\n",
    "    epsilon = 1e-7\n",
    "    pred = np.clip(pred, epsilon, 1 - epsilon)\n",
    "    loss = -np.sum(targets * np.log(pred)) / len(pred)\n",
    "    return loss\n",
    "\n",
    "def deriv_categorical_cross_entropy(a, targets):\n",
    "    return a - targets\n",
    "\n",
    "\n",
    "def normalize_data(X_train, X_test):\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    \n",
    "    std[std < 1e-8] = 1e-8\n",
    "\n",
    "    X_train_normalized = (X_train - mean) / std\n",
    "\n",
    "    X_test_normalized = (X_test - mean) / std\n",
    "    \n",
    "    return X_train_normalized, X_test_normalized\n",
    "\n",
    "def init_param(input_size, output_size, hidden_size, features):\n",
    "    w1 = np.random.randn(features, input_size) #3072x3072\n",
    "    b1 = np.random.randn(1, input_size) # 1x3072\n",
    "    w2 = np.random.randn(features, hidden_size) # 3072x200\n",
    "    b2 = np.random.randn(1, hidden_size) # 1x200\n",
    "    w3 = np.random.randn(hidden_size, output_size) # 200x10\n",
    "    b3 = np.random.randn(1, output_size) # 1x10\n",
    "    return w1, b1, w2, b2, w3, b3\n",
    "\n",
    "\n",
    "def forward_prop(w1, b1, w2, b2, w3, b3, X):\n",
    "    z1 = X.dot(w1) + b1 # mx3072 3072x3072 = mx3072\n",
    "    a1 = relu(z1) # mx3072\n",
    "    a1, _ = normalize_data(a1, a1)\n",
    "    z2 = a1.dot(w2) + b2 # mx3072 x 3072x200 = mx200\n",
    "    a2 = relu(z2) # mx200\n",
    "    a2, _ = normalize_data(a2, a2) \n",
    "    z3 = a2.dot(w3) + b3 # mx200 200x10 = mx10\n",
    "    a3 = softmax(z3) # mx10\n",
    "    return z1, a1, z2, a2, z3, a3\n",
    "    \n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((len(Y), np.unique(Y).size))\n",
    "    one_hot_Y[np.arange(len(Y)), Y] = 1\n",
    "    return one_hot_Y\n",
    "    \n",
    "    \n",
    "def back_prop(z1, a1, z2, a2, z3, a3, w2, w3, X, Y):\n",
    "    m = len(Y)\n",
    "    one_hot_Y = one_hot(Y) # mx10\n",
    "    dz3 = deriv_categorical_cross_entropy(a3, one_hot_Y) * deriv_softmax(a3) # mx10\n",
    "    dw3 = (1 / m) * (a2.T).dot(dz3) # 200xm mx10 = 200x10\n",
    "    db3 = (1 / m) * np.sum(dz3, axis=0) # 1x10\n",
    "    dz2 = dz3.dot(w3.T) * deriv_relu(z2) # mx10 10x200 * mx50 = mx200\n",
    "    dw2 = (1 / m) * (a1.T).dot(dz2) # 3072xm mx200 = 3072x200\n",
    "    db2 = (1 / m) * np.sum(dz2, axis=0) # 1x200\n",
    "    dz1 = dz2.dot(w2.T) * deriv_relu(z1) # mx20 200x3072 * mx3072 = mx3072\n",
    "    dw1 = (1 / m) * (X.T).dot(dz1) # 3072xm mx3072 = 3072x3072\n",
    "    db1 = (1 / m) * np.sum(dz1, axis=0) # 1x3072\n",
    "    return dw1, db1, dw2, db2, dw3, db3\n",
    "\n",
    "def update_params(w1, b1, w2, b2, w3, b3, dw1, db1, dw2, db2, dw3, db3, lr):\n",
    "    w1 -= lr * (dw1 + 0.01 * w1)\n",
    "    b1 -= lr * (db1 + 0.01 * b1)\n",
    "    w2 -= lr * (dw2 + 0.01 * w2)\n",
    "    b2 -= lr * (db2 + 0.01 * b2)\n",
    "    w3 -= lr * (dw3 + 0.01 * w3)\n",
    "    b3 -= lr * (db3 + 0.01 * b3)\n",
    "    return w1, b1, w2, b2, w3, b3\n",
    "\n",
    "def get_predictions(a):\n",
    "    return np.argmax(a, axis=1)\n",
    "    \n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / len(Y)\n",
    "\n",
    "def update_learning_rate(initial_learning_rate, epoch, decay = 0.1, epoch_update = 50):\n",
    "    return initial_learning_rate * decay ** (epoch // epoch_update)\n",
    "\n",
    "def gradient_descent(X, Y, X_test, Y_test, hidden_size=50, epoch = 200, epoch_update = 10, initial_learning_rate = 0.1, batch_size = 250):\n",
    "    X, X_test = normalize_data(X, X_test)\n",
    "    train_samples, features = X.shape\n",
    "    w1, b1, w2, b2, w3, b3 = init_param(features, len(np.unique(Y)), hidden_size, features)\n",
    "    \n",
    "    train_error = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    for i in range(epoch):\n",
    "        learning_rate = update_learning_rate(initial_learning_rate, (i + 1))\n",
    "        for j in range(int(train_samples / batch_size)):\n",
    "            X_train = X[batch_size*j : batch_size*(j+1)]\n",
    "            Y_train = Y[batch_size*j : batch_size*(j+1)]\n",
    "            z1, a1, z2, a2, z3, a3 = forward_prop(w1, b1, w2, b2, w3, b3, X_train)\n",
    "            dw1, db1, dw2, db2, dw3, db3 = back_prop(z1, a1, z2, a2, z3, a3, w2, w3, X_train, Y_train)\n",
    "            w1, b1, w2, b2, w3, b3 = update_params(w1, b1, w2, b2, w3, b3, dw1, db1, dw2, db2, dw3, db3, learning_rate)\n",
    "        if (i + 1) % epoch_update == 0:\n",
    "            print(f\"Epoch {i + 1} / {epoch}\")\n",
    "            _, _, _, _, _, a3 = forward_prop(w1, b1, w2, b2, w3, b3, X)\n",
    "            train_accuracy.append(get_accuracy(get_predictions(a3), Y))\n",
    "            cel = categorical_cross_entropy_loss(a3, one_hot(Y))\n",
    "            train_error.append(cel)\n",
    "            print(\"Train Error: \" + str(cel))\n",
    "            print(\"Train Accuracy: \" + str(train_accuracy[-1]))\n",
    "            _, _, _, _, _, a3 = forward_prop(w1, b1, w2, b2, w3, b3, X_test)\n",
    "            test_accuracy.append(get_accuracy(get_predictions(a3), Y_test))\n",
    "            print(\"Test Accuracy: \" + str(test_accuracy[-1]))\n",
    "\n",
    "    # Plot errors\n",
    "    fig_errors, ax_errors = plt.subplots(figsize=(12, 6))\n",
    "    ax_errors.plot(range(epoch_update, epoch + 1, epoch_update), train_error, label='Train Error', marker='o', linestyle='-')\n",
    "    ax_errors.set_xlabel('Epochs')\n",
    "    ax_errors.set_ylabel('Error')\n",
    "    ax_errors.set_title('Training Error Over Epochs (learning rate = 0.1)')\n",
    "    ax_errors.axvline(50, linestyle='--', color='gray', linewidth=1, label='Learning Rate = 0.01')\n",
    "    ax_errors.axvline(100, linestyle='--', color='gray', linewidth=1, label='Learning Rate = 0.001')\n",
    "    ax_errors.axvline(150, linestyle='--', color='gray', linewidth=1, label='Learning Rate = 0.0001')\n",
    "    ax_errors.legend()\n",
    "\n",
    "    # Plot accuracies in the same figure\n",
    "    fig_accuracies, ax_accuracies = plt.subplots(figsize=(12, 6))\n",
    "    ax_accuracies.plot(range(epoch_update, epoch + 1, epoch_update), train_accuracy, label='Train Accuracy', marker='o', linestyle='-', color='blue')\n",
    "    ax_accuracies.plot(range(epoch_update, epoch + 1, epoch_update), test_accuracy, label='Test Accuracy', marker='o', linestyle='-', color='red')\n",
    "    ax_accuracies.set_xlabel('Epochs')\n",
    "    ax_accuracies.set_ylabel('Accuracy')\n",
    "    ax_accuracies.set_title('Training and Test Accuracies Over Epochs (learning rate = 0.1)')\n",
    "    ax_errors.axvline(50, linestyle='--', color='gray', linewidth=1, label='Learning Rate = 0.01')\n",
    "    ax_errors.axvline(100, linestyle='--', color='gray', linewidth=1, label='Learning Rate = 0.001')\n",
    "    ax_errors.axvline(150, linestyle='--', color='gray', linewidth=1, label='Learning Rate = 0.0001')\n",
    "    ax_accuracies.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    train_paths = ('data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5')\n",
    "    test_path = 'test_batch'\n",
    "\n",
    "    train_data = {}\n",
    "    test_data = unpickle(test_path)\n",
    "\n",
    "    for i in train_paths:\n",
    "        train_data[i] = unpickle(i)\n",
    "\n",
    "    train_labels = np.concatenate([train_data[key][b'labels'] for key in train_data.keys()])\n",
    "    train_data = np.concatenate([train_data[key][b'data'] for key in train_data.keys()])\n",
    "\n",
    "    test_labels = test_data[b'labels']\n",
    "    test_data = test_data[b'data']\n",
    "    gradient_descent(train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ceec8-6e90-4cc6-9f92-5d7ca6f720cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
